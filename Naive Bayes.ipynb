{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "211dfed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81ca02af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv',nrows = 1000)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3e6720b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you enco...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000042bf85aa498cd78e</td>\n",
       "      <td>How did Otto von Guericke used the Magdeburg h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000455dfa3e01eae3af</td>\n",
       "      <td>Can I convert montra helicon D to a mountain b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid                                      question_text  \\\n",
       "0  00002165364db923c7e6  How did Quebec nationalists see their province...   \n",
       "1  000032939017120e6e44  Do you have an adopted dog, how would you enco...   \n",
       "2  0000412ca6e4628ce2cf  Why does velocity affect time? Does velocity a...   \n",
       "3  000042bf85aa498cd78e  How did Otto von Guericke used the Magdeburg h...   \n",
       "4  0000455dfa3e01eae3af  Can I convert montra helicon D to a mountain b...   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b7e64b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sincere questions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "792    How is your experience with husky dogs in Indi...\n",
       "294    Is laser a good option to remove these blavk l...\n",
       "738    What does the chairman of a regional federal r...\n",
       "532       What is the best diet for teddy bear hamsters?\n",
       "788    What do you think will happen in Seasons 5 and...\n",
       "Name: question_text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#taking a look at sincere Questions\n",
    "print(\"sincere questions\")\n",
    "train.loc[train['target']==0].sample(5)['question_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cb57a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "insincere questions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "996    Why Chinese people are always not welcome in a...\n",
       "615    Why do people on Quora not answer my question ...\n",
       "460    How much more political fumbling will it take ...\n",
       "803    Why are Europeans so ungrateful for America? T...\n",
       "30     Which babies are more sweeter to their parents...\n",
       "Name: question_text, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(insincere questions)\n",
    "print('insincere questions')\n",
    "train.loc[train['target']==1].sample(5)['question_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcbae386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do I find out the password for my Elite paycheck plus debit card if I haven’t set one up?\n"
     ]
    }
   ],
   "source": [
    "samp = train.sample(1)\n",
    "sentence = samp.iloc[0]['question_text']\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31212c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence after removing the numbers\n",
      " How do I find out the password for my Elite paycheck plus debit card if I haven’t set one up?\n"
     ]
    }
   ],
   "source": [
    "#preprocessing the text\n",
    "import re\n",
    "sentence = re.sub(r'\\d+','',sentence)\n",
    "print('Sentence after removing the numbers\\n',sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2399acdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{33: None,\n",
       " 34: None,\n",
       " 35: None,\n",
       " 36: None,\n",
       " 37: None,\n",
       " 38: None,\n",
       " 39: None,\n",
       " 40: None,\n",
       " 41: None,\n",
       " 42: None,\n",
       " 43: None,\n",
       " 44: None,\n",
       " 45: None,\n",
       " 46: None,\n",
       " 47: None,\n",
       " 58: None,\n",
       " 59: None,\n",
       " 60: None,\n",
       " 61: None,\n",
       " 62: None,\n",
       " 63: None,\n",
       " 64: None,\n",
       " 91: None,\n",
       " 92: None,\n",
       " 93: None,\n",
       " 94: None,\n",
       " 95: None,\n",
       " 96: None,\n",
       " 123: None,\n",
       " 124: None,\n",
       " 125: None,\n",
       " 126: None}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing the punctuation marks\n",
    "import string\n",
    "translation_table = sentence.maketrans(\"\",\"\",string.punctuation)\n",
    "translation_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2036610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How do I find out the password for my Elite paycheck plus debit card if I haven’t set one up'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = sentence.translate(translation_table)\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d372cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'card', 'find', 'debit', 'How', 'set', 'Elite', 'paycheck', 'plus', 'password', 'haven’t', 'I']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Prince\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Prince\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#removing stopwords from the sentence\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "words_in_sentence = list(set(sentence.split())-stop_words)\n",
    "print(words_in_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "164f7b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'card', 'find', 'debit', 'how', 'set', 'elit', 'paycheck', 'plu', 'password', 'haven’t', 'i']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Prince\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#stemming of words\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('wordnet')\n",
    "stemmer = PorterStemmer()\n",
    "for i,word in enumerate(words_in_sentence):\n",
    "    words_in_sentence[i]=stemmer.stem(word)\n",
    "print(words_in_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4420fca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'card', 'find', 'debit', 'how', 'set', 'elit', 'paycheck', 'plu', 'password', 'haven’t', 'i']\n"
     ]
    }
   ],
   "source": [
    "#lemmatization of words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = []\n",
    "for i,word in enumerate(words_in_sentence):\n",
    "    words_in_sentence[i] = lemmatizer.lemmatize(word)\n",
    "print(words_in_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d62b5d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the train data\n",
    "from sklearn.model_selection import train_test_split\n",
    "train,test = train_test_split(train,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de14ac55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "word_count={}\n",
    "word_count_sincere={}\n",
    "word_count_insincere={}\n",
    "sincere=0\n",
    "insincere= 0\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "stop_words  =set(nltk.corpus.stopwords.words('english'))\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "row_count = train.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "for row in range(0,row_count):\n",
    "    insincere += train.iloc[row]['target']\n",
    "    sincere += (1-train.iloc[row]['target'])\n",
    "    sentence = train.iloc[row]['question_text']\n",
    "    sentence = re.sub(r'\\d+',\"\",sentence)\n",
    "    translation_table = sentence.maketrans(\"\",\"\",string.punctuation)\n",
    "    sentence = sentence.translate(translation_table)\n",
    "    words_in_sentence = list((set(sentence.split())-stop_words))\n",
    "    for index,word in enumerate(words_in_sentence):\n",
    "        word = stemmer.stem(word)\n",
    "        words_in_sentence[index] =lemmatizer.lemmatize(word) \n",
    "    for word in words_in_sentence:\n",
    "        if train.iloc[row]['target']==0:  #sincere words\n",
    "            if word in word_count_sincere.keys():\n",
    "                word_count_sincere[word]+=1\n",
    "            else:\n",
    "                word_count_sincere[word]=1\n",
    "        elif train.iloc[row]['target']==1:  #insincere words\n",
    "            if word in word_count_insincere.keys():\n",
    "                word_count_insincere[word]+=1\n",
    "            else:\n",
    "                word_count_insincere[word]=1\n",
    "        if word in word_count.keys():\n",
    "            word_count[word]+=1\n",
    "        else:\n",
    "            word_count[word]=1\n",
    "            \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd953eb9",
   "metadata": {},
   "source": [
    "- Finding probability for each word in the dictionary. After that eliminating words which are insignificant. Insignificant words are words which have a probability of occurence less than 0.0001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "afc2afb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Words 2490\n",
      "Minimum probability 0.0001691761123329386\n",
      "Total Words 2490\n"
     ]
    }
   ],
   "source": [
    "word_probability = {}\n",
    "total_words = 0\n",
    "for i in word_count:\n",
    "    total_words +=word_count[i]\n",
    "for i in word_count:\n",
    "    word_probability[i] = word_count[i]/total_words\n",
    "    \n",
    "#Eliminating the insignificant words\n",
    "print('Total Words',len(word_probability))\n",
    "print('Minimum probability',min(word_probability.values()))\n",
    "threshold_p = 0.0001\n",
    "for i in list(word_probability):\n",
    "    if word_probability[i]<threshold_p:\n",
    "        del word_prbability[i]\n",
    "        if i in list(word_count_sincere):\n",
    "            del word_count_sincere[i]\n",
    "        elif i in list(word_count_insincere):\n",
    "            del word_count_insincere[i]\n",
    "print('Total Words',len(word_probability))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4889f166",
   "metadata": {},
   "source": [
    "- To apply naive bayes algorithm, we have to find conditional probability. Finding the conditional probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bbb4caa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sincere_words = sum(word_count_sincere.values())\n",
    "cp_sincere={}  #conditional probability\n",
    "for i in list(word_count_sincere):\n",
    "    cp_sincere[i] = word_count_sincere[i]/total_sincere_words\n",
    "    \n",
    "total_insincere_words =sum(word_count_insincere.values())\n",
    "cp_insincere ={}\n",
    "for i in list(word_count_insincere):\n",
    "    cp_insincere[i] = word_count_insincere[i]/total_insincere_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc3d6127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is  12.0\n"
     ]
    }
   ],
   "source": [
    "row_count=  test.shape[0]\n",
    "p_sincere = sincere/(sincere+insincere)\n",
    "p_insincere = insincere/(sincere+insincere)\n",
    "accuracy = 0\n",
    "\n",
    "for row in range(0,row_count):\n",
    "    sentence  = test.iloc[row]['question_text']\n",
    "    target = test.iloc[row]['target']\n",
    "    sentence = re.sub(r'\\d+','',sentence)\n",
    "    sentence = sentence.translate(sentence.maketrans(\"\",\"\",string.punctuation))\n",
    "    words_in_sentence = list(set(sentence.split())-stop_words)\n",
    "    for index,word in enumerate(words_in_sentence):\n",
    "        word = stemmer.stem(word)\n",
    "        words_in_sentence[index] = lemmatizer.lemmatize(word)\n",
    "    insincere_term = p_insincere\n",
    "    sincere_term = p_sincere\n",
    "    \n",
    "    sincere_M = len(cp_sincere.keys())\n",
    "    insincere_M = len(cp_insincere.keys())\n",
    "    for word in words_in_sentence:\n",
    "        if word not in cp_insincere.keys():\n",
    "            insincere_M+=1\n",
    "        if word not in cp_sincere.keys():\n",
    "            sincere_M+=1\n",
    "            \n",
    "    for word in words_in_sentence:\n",
    "        if word in cp_insincere.keys():\n",
    "            insincere_term *= (cp_insincere[word] + (1/insincere_M))\n",
    "        else:\n",
    "            insincere_term *= (1/insincere_M)\n",
    "        if word in cp_sincere.keys():\n",
    "            sincere_term *= (cp_sincere[word] + (1/sincere_M))\n",
    "        else:\n",
    "            sincere_term *= (1/sincere_M)\n",
    "        \n",
    "    if insincere_term/(insincere_term + sincere_term) > 0.5:\n",
    "        response = 1\n",
    "    else:\n",
    "        response = 0\n",
    "    if target == response:\n",
    "        accuracy += 1\n",
    "    \n",
    "print ('Accuracy is ',accuracy/row_count*100)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4de3a7",
   "metadata": {},
   "source": [
    "- Here we have manually found out the accuracy using the calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f903d3",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eed7c787",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8a6d86cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you enco...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000042bf85aa498cd78e</td>\n",
       "      <td>How did Otto von Guericke used the Magdeburg h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000455dfa3e01eae3af</td>\n",
       "      <td>Can I convert montra helicon D to a mountain b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid                                      question_text  \\\n",
       "0  00002165364db923c7e6  How did Quebec nationalists see their province...   \n",
       "1  000032939017120e6e44  Do you have an adopted dog, how would you enco...   \n",
       "2  0000412ca6e4628ce2cf  Why does velocity affect time? Does velocity a...   \n",
       "3  000042bf85aa498cd78e  How did Otto von Guericke used the Magdeburg h...   \n",
       "4  0000455dfa3e01eae3af  Can I convert montra helicon D to a mountain b...   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv',nrows=2000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b863a82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the feature and target column\n",
    "X = df['question_text']\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1743f154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000,), (2000,))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "876ec9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
      "Document-Term Matrix (DTM):\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]\n",
    "\n",
    "# Create a CountVectorizer instance\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the documents into a document-term matrix (DTM)\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# # Get the vocabulary (unique words) and the DTM as a dense array\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "dtm_dense = X.toarray()\n",
    "\n",
    "# # Print the results\n",
    "print(\"Vocabulary:\", vocabulary)\n",
    "print(\"Document-Term Matrix (DTM):\")\n",
    "print(dtm_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0b467a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lammetizer = WordNetLemmatizer()\n",
    "def pre_processing_text(sentence):\n",
    "    sentence = re.sub(r'\\d+',\"\",sentence) #remove numbers\n",
    "    sentence  = sentence.translate(sentence.maketrans(\"\",\"\",string.punctuation)) #remove punctuation\n",
    "    words = set(sentence.split())\n",
    "    words = list(words - stop_words)\n",
    "    words = ' '.join\n",
    "    for index,word in enumerate(words):\n",
    "        word = stemmer.stem(word)\n",
    "        words[index]=lammetizer.lemmatize(word)\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5fd3c39a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    How did Quebec nationalists see their province...\n",
       "1    Do you have an adopted dog, how would you enco...\n",
       "2    Why does velocity affect time? Does velocity a...\n",
       "3    How did Otto von Guericke used the Magdeburg h...\n",
       "4    Can I convert montra helicon D to a mountain b...\n",
       "Name: question_text, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "891c0dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How did Quebec nationalists see their province as a nation in the 1960s?'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7b9ac53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prince\\AppData\\Local\\Temp\\ipykernel_4728\\905917026.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[i] = pre_processing_text(X[i])\n"
     ]
    }
   ],
   "source": [
    "for i in range(X.shape[0]):\n",
    "    X[i] = pre_processing_text(X[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5c29a01f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            nationalist how quebec provinc nation see\n",
       "1         adopt adopt peopl dog would shop encourag do\n",
       "2             whi geometri veloc doe space time affect\n",
       "3         magdeburg guerick how hemispher use otto von\n",
       "4    can mountain helicon montra bike chang tyre co...\n",
       "Name: question_text, dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d6a294d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000,)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f55991ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1500,), (1500,))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating countvectorizer to convert the data into bag of words \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    " \n",
    "vectorizer = CountVectorizer()\n",
    "#splitting the data\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25)\n",
    "X_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c711bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9c6ee314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating Multinomial NB model\n",
    "NB = MultinomialNB()\n",
    "NB.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d63d5ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.936"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#making predictions on y_test\n",
    "y_pred= NB.predict(X_test)\n",
    "accuracy = accuracy_score(y_test,y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970cf152",
   "metadata": {},
   "source": [
    "- Model has 93.6 % accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c271ef08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prince\\AppData\\Local\\Temp\\ipykernel_4728\\1007605482.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[i] = pre_processing_text(X[i])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.946"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "df = pd.read_csv('train.csv',nrows=2000)\n",
    "#creating the feature and target column\n",
    "X = df['question_text']\n",
    "y = df['target']\n",
    "stemmer = PorterStemmer()\n",
    "lammetizer = WordNetLemmatizer()\n",
    "def pre_processing_text(sentence):\n",
    "    sentence = re.sub(r'\\d+',\"\",sentence) #remove numbers\n",
    "    sentence  = sentence.translate(sentence.maketrans(\"\",\"\",string.punctuation)) #remove punctuation\n",
    "    words = set(sentence.split())\n",
    "    words = list(words - stop_words)\n",
    "    for index,word in enumerate(words):\n",
    "        word = stemmer.stem(word)\n",
    "        words[index]=lammetizer.lemmatize(word)\n",
    "    return ' '.join(words)\n",
    "for i in range(X.shape[0]):\n",
    "    X[i] = pre_processing_text(X[i])\n",
    "vectorizer = CountVectorizer()\n",
    "#splitting the data\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25)\n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "#Creating Multinomial NB model\n",
    "NB = MultinomialNB()\n",
    "NB.fit(X_train,y_train)\n",
    "\n",
    "#making predictions on y_test\n",
    "y_pred= NB.predict(X_test)\n",
    "accuracy = accuracy_score(y_test,y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9f5d578b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               nationalist how quebec provinc nation see\n",
       "1            adopt adopt peopl dog would shop encourag do\n",
       "2                whi geometri veloc doe space time affect\n",
       "3            magdeburg guerick how hemispher use otto von\n",
       "4       can mountain helicon montra bike chang tyre co...\n",
       "                              ...                        \n",
       "1995                               what love two guy time\n",
       "1996            what header benefit c file function defin\n",
       "1997                         what rout line number bu brt\n",
       "1998    left whi told peopl explicitli twice harri ron...\n",
       "1999                        can ever nigeria nation great\n",
       "Name: question_text, Length: 2000, dtype: object"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dd362cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nationalist', 'how', 'quebec', 'provinc', 'nation', 'see']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].split()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
